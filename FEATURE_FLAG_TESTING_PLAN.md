# Feature Flag Testing Plan and Tracking Document
## Project Athena LLM Pipeline Feature Flag Validation

**Created:** 2025-11-18
**Status:** In Progress
**Last Updated:** 2025-11-18

## Executive Summary

This document tracks comprehensive testing of all feature flags in Project Athena's LLM pipeline. The goal is to ensure:
1. Each feature flag works correctly when enabled
2. Each feature flag can be safely disabled
3. The admin UI has real control over system configuration
4. All issues are documented and resolved

## Feature Flags Inventory

### LLM Processing Features

| ID | Name | Display Name | Category | Current State | Required | Description |
|----|------|--------------|----------|---------------|----------|-------------|
| 1 | intent_classification | Intent Classification | processing | ‚úÖ Enabled | Yes | Classify user query intent before processing |
| 2 | multi_intent_detection | Multi-Intent Detection | processing | ‚úÖ Enabled | No | Detect and handle multiple intents in single query |
| 3 | conversation_context | Conversation Context | processing | ‚úÖ Enabled | No | Track conversation history for contextual responses |
| 12 | llm_based_routing | Use LLM for Intent Classification | routing | ‚úÖ Enabled | No | Use AI to intelligently classify query intent instead of keyword matching |
| 13 | enable_llm_intent_classification | LLM Intent Classification | llm | ‚úÖ Enabled | No | Use LLM (phi3:mini) for intent classification in Orchestrator |

### RAG Services

| ID | Name | Display Name | Category | Current State | Required | Description |
|----|------|--------------|----------|---------------|----------|-------------|
| 4 | rag_weather | Weather RAG | rag | ‚úÖ Enabled | No | Retrieve weather information from APIs (150ms avg latency) |
| 5 | rag_sports | Sports RAG | rag | ‚úÖ Enabled | No | Retrieve sports scores and updates (120ms avg latency) |
| 6 | rag_airports | Airport RAG | rag | ‚úÖ Enabled | No | Retrieve flight and airport information (140ms avg latency) |

### Optimization Features

| ID | Name | Display Name | Category | Current State | Required | Description |
|----|------|--------------|----------|---------------|----------|-------------|
| 7 | redis_caching | Redis Caching | optimization | ‚úÖ Enabled | No | Cache frequently accessed data in Redis (-30ms latency improvement) |
| 8 | mlx_backend | MLX Backend | optimization | ‚ùå Disabled | No | Use Apple Silicon MLX for faster inference (-100ms latency improvement) |
| 9 | response_streaming | Response Streaming | optimization | ‚úÖ Enabled | No | Stream responses for better perceived latency (-20ms improvement) |

### Integration Features

| ID | Name | Display Name | Category | Current State | Required | Description |
|----|------|--------------|----------|---------------|----------|-------------|
| 10 | home_assistant | Home Assistant Integration | integration | ‚úÖ Enabled | No | Control smart home devices via Home Assistant (200ms avg latency) |
| 11 | clarifications | Clarification Prompts | integration | ‚úÖ Enabled | No | Ask for clarification when intent is ambiguous (50ms avg latency) |

## Testing Plan

### Phase 1: LLM Intent Classification (HIGH PRIORITY)

#### Test 1.1: enable_llm_intent_classification (Enabled State)

**Objective:** Verify LLM-based classification works correctly
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Verify flag is enabled: `curl -s "https://athena-admin.xmojo.net/api/features/public" | jq '.[] | select(.name=="enable_llm_intent_classification")'`
2. Send test query to orchestrator: "What's the weather in Baltimore?"
3. Check orchestrator logs for "Using simplified LLM classification"
4. Verify response contains weather information
5. Measure latency (should be 50-200ms added)

**Expected Results:**
- ‚úÖ LLM classification logs appear
- ‚úÖ Intent classified correctly (WEATHER)
- ‚úÖ Response contains actual weather data
- ‚úÖ Latency within acceptable range

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

#### Test 1.2: enable_llm_intent_classification (Disabled State)

**Objective:** Verify pattern-based classification fallback works
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Disable flag via admin API or UI
2. Verify flag is disabled
3. Send same test query: "What's the weather in Baltimore?"
4. Check orchestrator logs for "Using standard JSON-based LLM classification"
5. Verify response still works (fallback to pattern matching)

**Expected Results:**
- ‚úÖ Pattern-based classification logs appear
- ‚úÖ Intent still classified correctly
- ‚úÖ Response contains weather data
- ‚úÖ Latency lower (no LLM overhead)

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Phase 2: RAG Services Testing

#### Test 2.1: rag_weather (Enabled State)

**Objective:** Verify weather RAG service integration
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Verify flag is enabled
2. Send weather query: "What's the weather today?"
3. Check orchestrator logs for weather RAG service call
4. Verify response contains real weather data (not hallucinations)
5. Check for proper fallback to web search if RAG fails

**Expected Results:**
- ‚úÖ Weather RAG service called
- ‚úÖ Response contains current weather data
- ‚úÖ Data source attribution present
- ‚úÖ Fallback to web search if RAG unavailable

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

#### Test 2.2: rag_weather (Disabled State)

**Objective:** Verify weather queries work without RAG service
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Disable rag_weather flag
2. Send weather query: "What's the weather today?"
3. Verify system falls back to web search or LLM knowledge
4. Verify response acknowledges lack of real-time data

**Expected Results:**
- ‚úÖ No RAG service call in logs
- ‚úÖ Web search fallback activates
- ‚úÖ Response indicates data source

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

#### Test 2.3: rag_sports (Enabled State)

**Objective:** Verify sports RAG service integration
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Verify flag is enabled
2. Send sports query: "When do the Ravens play next?"
3. Check orchestrator logs for sports RAG service call
4. Verify response contains actual game schedule
5. Verify team name extraction works

**Expected Results:**
- ‚úÖ Sports RAG service called
- ‚úÖ Response contains game schedule
- ‚úÖ Team correctly identified (Ravens)
- ‚úÖ No hallucinated game data

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

#### Test 2.4: rag_sports (Disabled State)

**Objective:** Verify sports queries work without RAG service
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Disable rag_sports flag
2. Send sports query: "When do the Ravens play next?"
3. Verify system falls back appropriately
4. Verify no hallucinated sports data

**Expected Results:**
- ‚úÖ No RAG service call
- ‚úÖ Fallback behavior activates
- ‚úÖ Response acknowledges lack of current data

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

#### Test 2.5: rag_airports (Enabled/Disabled States)

**Objective:** Verify airport RAG service integration
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:** (Similar to weather and sports tests)

**Expected Results:** (To be documented)

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Phase 3: Optimization Features

#### Test 3.1: redis_caching (Enabled State)

**Objective:** Verify caching improves performance
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Verify flag is enabled
2. Send query twice: "What's the weather today?"
3. Check logs for cache hit on second request
4. Measure latency difference (first vs second)

**Expected Results:**
- ‚úÖ First request: cache miss
- ‚úÖ Second request: cache hit
- ‚úÖ Latency reduction on cache hit (-30ms)

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

#### Test 3.2: redis_caching (Disabled State)

**Objective:** Verify system works without caching
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Disable redis_caching flag
2. Send query twice
3. Verify no cache hits in logs
4. Verify full processing each time

**Expected Results:**
- ‚úÖ No cache operations
- ‚úÖ Full latency every time
- ‚úÖ Correct responses still generated

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Phase 4: Conversation Context

#### Test 4.1: conversation_context (Enabled State)

**Objective:** Verify conversation history tracking works
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Verify flag is enabled
2. Create new session, send query: "What's the weather?"
3. Send follow-up: "How about tomorrow?"
4. Verify second query uses context from first
5. Check logs for conversation history inclusion

**Expected Results:**
- ‚úÖ Session created
- ‚úÖ Follow-up query understands context
- ‚úÖ Response references previous question

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

#### Test 4.2: conversation_context (Disabled State)

**Objective:** Verify queries work without conversation tracking
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Disable conversation_context flag
2. Send query: "What's the weather?"
3. Send follow-up: "How about tomorrow?"
4. Verify second query doesn't use context

**Expected Results:**
- ‚úÖ No session tracking
- ‚úÖ Each query treated independently
- ‚úÖ Follow-up query fails or asks for clarification

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Phase 5: Admin UI Control Verification

**Objective:** Verify admin UI can toggle all flags
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Access admin UI: https://athena-admin.xmojo.net/#features
2. For each non-required feature flag:
   - Toggle off via UI
   - Verify API shows updated state
   - Verify orchestrator respects new state (within 60s cache TTL)
   - Toggle back on
   - Verify everything still works

**Features to Test:**
- enable_llm_intent_classification
- llm_based_routing
- multi_intent_detection
- conversation_context
- rag_weather
- rag_sports
- rag_airports
- redis_caching
- mlx_backend (test enabling it)
- response_streaming
- home_assistant
- clarifications

**Expected Results:**
- ‚úÖ All toggles work in UI
- ‚úÖ API reflects changes immediately
- ‚úÖ Orchestrator picks up changes within cache TTL (60s)

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

## Test Execution Log

### Session 1: Initial Testing (2025-11-18)

**Environment:**
- Mac Studio (192.168.10.167)
- Orchestrator: Port 8001
- Gateway: Port 8000
- Admin API: https://athena-admin.xmojo.net

**Tests Executed:**
1. (To be filled during testing)

**Issues Discovered:**
1. (To be filled during testing)

**Fixes Applied:**
1. (To be filled during testing)

---

## Issues Tracker

### Critical Issues

**ISSUE #1: Cannot toggle feature flags programmatically for testing**
- **Status:** üü° PARTIALLY RESOLVED
- **Impact:** Cannot test disabled states of feature flags without deployment
- **Root Cause:**
  - Admin API toggle endpoint requires OIDC authentication
  - Service API key (X-API-Key header) not accepted by original toggle endpoint
  - Database not accessible from test environment (postgres-01.xmojo.net connection issues)
  - Admin backend running in kubernetes cluster that's not accessible
- **Attempted Solutions:**
  1. ‚ùå Direct database access via psql - Connection refused
  2. ‚ùå Python script with DATABASE_URL - Authentication failed
  3. ‚ùå Service API key authentication - Not authenticated error
- **Solution Implemented:**
  1. ‚úÖ **Created `/api/features/service/{feature_id}/toggle` endpoint** (admin/backend/app/routes/features.py)
     - Accepts X-API-Key header for service-to-service auth
     - Uses SERVICE_API_KEY from environment
     - Mirrors functionality of OIDC-protected toggle endpoint
     - Code change complete and verified
  2. üü° **Deployment Pending** - Requires Docker build and kubernetes rollout
     - Need to build admin backend Docker image
     - Push to container registry
     - Apply kubectl rollout restart
     - Estimated time: 10-15 minutes
- **Next Steps:**
  1. ‚úÖ Service toggle endpoint code complete
  2. ‚è∏Ô∏è Deploy to kubernetes (can be done later)
  3. ‚úÖ Continue with tests that work with currently enabled flags
  4. ‚úÖ Test flag toggling once deployment complete

### High Priority Issues
(None yet)

### Medium Priority Issues
(None yet)

### Low Priority Issues
(None yet)

### Resolved Issues
(None yet)

---

## Test Queries Reference

### Weather Queries
- "What's the weather today?"
- "What's the weather in Baltimore?"
- "Will it rain tomorrow?"
- "What's the temperature outside?"

### Sports Queries
- "When do the Ravens play next?"
- "What's the Ravens score?"
- "Did the Orioles win yesterday?"
- "Ravens schedule this week"

### Airport Queries
- "BWI flight delays?"
- "Are there delays at BWI?"
- "Flight status for BWI"

### Multi-Intent Queries
- "What's the weather and when do the Ravens play?"
- "Check the weather then tell me the Ravens score"

### Conversation Context Queries
- Query 1: "What's the weather?"
- Query 2 (follow-up): "How about tomorrow?"
- Query 3 (follow-up): "And the day after?"

---

## Completion Criteria

### System is "Working as Designed" when:
- [ ] All non-required feature flags can be toggled on/off without errors
- [ ] Enabling a flag activates the feature
- [ ] Disabling a flag gracefully falls back to alternative behavior
- [ ] Admin UI accurately reflects and controls flag states
- [ ] Orchestrator respects flag changes within cache TTL
- [ ] No hallucinations occur regardless of flag states
- [ ] All RAG services work or fall back gracefully
- [ ] Caching improves performance when enabled
- [ ] Conversation context works when enabled
- [ ] Zero critical or high-priority bugs remain

### Testing Complete When:
- [ ] All 13 feature flags tested in both enabled/disabled states (except required flags)
- [ ] All test queries execute successfully
- [ ] All issues discovered are documented and resolved
- [ ] Admin UI control verified for all flags
- [ ] Tracking document complete and accurate
- [ ] Handoff document created for continuity

---

---

## Phase 6: Non-Flag Features Testing

### Test 6.1: Anti-Hallucination Validation

**Objective:** Verify response validation prevents hallucinations
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Send query that has NO real data: "What time does the unicorn store open?"
2. Verify validator detects lack of supporting data
3. Verify response acknowledges uncertainty
4. Send query WITH data: "What's the weather?"
5. Verify response contains specific facts WITH data support

**Expected Results:**
- ‚úÖ Queries without data don't generate specific facts
- ‚úÖ Validation layer catches hallucinations
- ‚úÖ Responses acknowledge limitations appropriately

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Test 6.2: Parallel Web Search

**Objective:** Verify parallel search across providers works
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Send general knowledge query: "What is quantum computing?"
2. Check logs for parallel search execution
3. Verify multiple providers queried (DuckDuckGo, Brave, etc.)
4. Verify result fusion combines results correctly
5. Measure latency improvement vs sequential search

**Expected Results:**
- ‚úÖ Multiple providers queried in parallel
- ‚úÖ Results fused and ranked appropriately
- ‚úÖ Latency better than sequential
- ‚úÖ Sources properly attributed

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Test 6.3: Session Management

**Objective:** Verify session creation, persistence, and cleanup
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Send query without session_id
2. Verify new session created
3. Send follow-up with session_id
4. Verify session retrieved and updated
5. Check session TTL and cleanup
6. Test session export endpoints

**Expected Results:**
- ‚úÖ Sessions auto-created when needed
- ‚úÖ Session history persists correctly
- ‚úÖ TTL cleanup works (default 1 hour)
- ‚úÖ Export endpoints return correct data

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Test 6.4: LangGraph State Machine Flow

**Objective:** Verify all nodes execute correctly
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Test CONTROL path: "Turn on the office lights"
   - Verify: classify ‚Üí route_control ‚Üí finalize
2. Test INFO path: "What's the weather?"
   - Verify: classify ‚Üí route_info ‚Üí retrieve ‚Üí synthesize ‚Üí validate ‚Üí finalize
3. Test UNKNOWN path: "Blah blah gibberish"
   - Verify: classify ‚Üí finalize (short circuit)
4. Check node timings in response metadata

**Expected Results:**
- ‚úÖ Correct path taken for each intent
- ‚úÖ All nodes execute without errors
- ‚úÖ Node timings captured
- ‚úÖ Proper error handling

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Test 6.5: Model Tier Selection

**Objective:** Verify correct model chosen for query complexity
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Send simple query: "What's 2+2?"
   - Verify: phi3:mini (SMALL) selected
2. Send medium query: "Explain how cars work"
   - Verify: llama3.1:8b (MEDIUM) selected
3. Send complex query: "Write a detailed analysis of quantum physics"
   - Verify: llama3.1:8b (LARGE) selected
4. Check metadata for model_used field

**Expected Results:**
- ‚úÖ Simple queries use SMALL model
- ‚úÖ Complex queries use LARGE model
- ‚úÖ Model selection logged
- ‚úÖ Correct model appears in metadata

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Test 6.6: Fallback Mechanisms

**Objective:** Verify graceful degradation when services fail
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Stop weather RAG service
2. Send weather query
3. Verify fallback to web search
4. Verify response indicates fallback source
5. Stop all RAG services
6. Verify fallback to LLM knowledge
7. Restart services and verify recovery

**Expected Results:**
- ‚úÖ RAG failure triggers web search
- ‚úÖ Web search failure triggers LLM knowledge
- ‚úÖ Fallback sources clearly indicated
- ‚úÖ No user-facing errors
- ‚úÖ System recovers when services restart

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Test 6.7: LLM Backend Routing (Ollama vs MLX)

**Objective:** Verify database-driven LLM backend selection
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Check current LLM backend configuration
2. Verify Ollama backend active
3. Send test query, verify Ollama used
4. Enable MLX backend (if available)
5. Send test query, verify MLX used
6. Test "auto" mode selects best backend

**Expected Results:**
- ‚úÖ Backend configuration loaded from database
- ‚úÖ Correct backend selected for each query
- ‚úÖ Auto mode selects based on performance metrics

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Test 6.8: Error Handling and Edge Cases

**Objective:** Verify robust error handling
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Send malformed query (empty string)
2. Send very long query (>1000 words)
3. Send query with special characters
4. Test network timeouts (mock slow service)
5. Test invalid session_id
6. Test concurrent requests (load testing)

**Expected Results:**
- ‚úÖ Empty queries handled gracefully
- ‚úÖ Long queries processed or rejected safely
- ‚úÖ Special characters don't break parsing
- ‚úÖ Timeouts trigger fallbacks
- ‚úÖ Invalid session_ids create new sessions
- ‚úÖ Concurrent requests handled correctly

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Test 6.9: Metrics and Observability

**Objective:** Verify metrics collection and endpoints
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Check /metrics endpoint (Prometheus format)
2. Check /llm-metrics endpoint (performance data)
3. Send multiple queries
4. Verify metrics updated correctly
5. Check request counters
6. Check latency histograms
7. Verify node timing data captured

**Expected Results:**
- ‚úÖ Metrics endpoints accessible
- ‚úÖ Counters increment correctly
- ‚úÖ Histograms capture latency distributions
- ‚úÖ LLM performance metrics accurate

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

## Phase 7: End-to-End Integration Tests

### Test 7.1: Complete User Workflows

**Objective:** Test realistic multi-turn conversations
**Status:** ‚è∏Ô∏è Not Started

**Test Scenarios:**

**Scenario A: Weather Planning**
1. "What's the weather today?"
2. "How about tomorrow?"
3. "Should I bring an umbrella?"

**Scenario B: Sports Fan**
1. "When do the Ravens play next?"
2. "What's their record this season?"
3. "Who are they playing?"

**Scenario C: Travel Planning**
1. "Are there delays at BWI?"
2. "What's the weather in Miami?"
3. "Should I leave early for the airport?"

**Expected Results:**
- ‚úÖ Context maintained across turns
- ‚úÖ All data retrieved accurately
- ‚úÖ No hallucinations
- ‚úÖ Natural conversation flow

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

### Test 7.2: Performance Benchmarks

**Objective:** Measure end-to-end performance
**Status:** ‚è∏Ô∏è Not Started

**Test Steps:**
1. Send 100 test queries (mix of types)
2. Measure P50, P95, P99 latency
3. Verify latency targets met:
   - Simple queries: <1s
   - RAG queries: <2s
   - Complex queries: <5s
4. Check resource utilization (CPU, memory)

**Expected Results:**
- ‚úÖ P95 latency meets targets
- ‚úÖ No memory leaks
- ‚úÖ CPU utilization reasonable
- ‚úÖ Throughput acceptable

**Actual Results:** (To be filled during testing)

**Issues Found:** (To be filled during testing)

---

## Next Steps

1. ‚úÖ Create comprehensive test plan covering ALL features
2. ‚è∏Ô∏è Begin Phase 1: Test enable_llm_intent_classification flag
3. ‚è∏Ô∏è Continue through all phases systematically
4. ‚è∏Ô∏è Fix issues immediately as discovered
5. ‚è∏Ô∏è Document all results in this tracking file
6. ‚è∏Ô∏è DO NOT STOP until all tests pass and all issues resolved
7. ‚è∏Ô∏è Create final summary report

---

## Handoff Information

**For Next Agent/Session:**
- This document contains complete test plan and results
- Check "Test Execution Log" for current progress
- Check "Issues Tracker" for known problems
- Each test has clear steps and expected results
- Update tracking as you complete tests

**Key Files:**
- This document: `/Users/jaystuart/dev/project-athena/FEATURE_FLAG_TESTING_PLAN.md`
- Orchestrator code: `src/orchestrator/main.py`
- Admin config client: `src/shared/admin_config.py`
- Feature flags API: `https://athena-admin.xmojo.net/api/features/public`

**Access Information:**
- Mac Studio SSH: `ssh jstuart@192.168.10.167`
- Orchestrator logs: `ssh jstuart@192.168.10.167 "tail -100 ~/dev/project-athena/orchestrator.log"`
- Admin UI: `https://athena-admin.xmojo.net/#features`
